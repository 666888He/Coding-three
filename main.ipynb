{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKSRbBnbFpxh"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "T9DDvk9FC2a-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lBeGN4BTEYm"
      },
      "source": [
        "# Load data from image files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "GxnnSo5yC7hw"
      },
      "outputs": [],
      "source": [
        "def image_to_data(image_folder='/content/drive/MyDrive/dcgan/Landscape'):\n",
        "    target_size = 512\n",
        "    data = []\n",
        "    count = 0\n",
        "    for filename in os.listdir(image_folder):\n",
        "        if count >= 500:\n",
        "            break\n",
        "        image_path = os.path.join(image_folder, filename)\n",
        "        image = Image.open(image_path)\n",
        "        width, height = image.size\n",
        "        max_left = width - target_size\n",
        "        max_top = height - target_size\n",
        "        if max_left < 0 or max_top < 0:\n",
        "            continue\n",
        "        left = random.randint(0, max_left)\n",
        "        top = random.randint(0, max_top)\n",
        "        right = left + target_size\n",
        "        bottom = top + target_size\n",
        "        cropped_image = image.crop((left, top, right, bottom))\n",
        "        resized_image = cropped_image.resize((target_size, target_size))\n",
        "        image_array = np.array(resized_image)\n",
        "        if len(image_array.shape) < 3 or image_array.shape[-1] != 3:\n",
        "            continue\n",
        "        data.append(image_array[np.newaxis, :, :, :])\n",
        "\n",
        "        count += 1\n",
        "    data = np.concatenate(data, axis=0)\n",
        "    return data\n",
        "\n",
        "X_train = image_to_data()\n",
        "X_train = (X_train.astype('float32')) / 255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvtzBQrySMer"
      },
      "source": [
        "# Build a DCGAN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "FQ4LwLXASE8M"
      },
      "outputs": [],
      "source": [
        "def build_discriminator():\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Conv2D(8, kernel_size=4, strides=2, padding=\"same\", input_shape=(512, 512, 3)))\n",
        "    model.add(layers.LeakyReLU(alpha=0.01))\n",
        "    model.add(layers.Conv2D(16, kernel_size=4, strides=2, padding=\"same\"))\n",
        "    model.add(layers.LeakyReLU(alpha=0.01))\n",
        "    model.add(layers.Conv2D(32, kernel_size=4, strides=2, padding=\"same\"))\n",
        "    model.add(layers.LeakyReLU(alpha=0.01))\n",
        "    model.add(layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"))\n",
        "    model.add(layers.LeakyReLU(alpha=0.01))\n",
        "    model.add(layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"))\n",
        "    model.add(layers.LeakyReLU(alpha=0.01))\n",
        "    model.add(layers.Conv2D(256, kernel_size=4, strides=2, padding=\"same\"))\n",
        "    model.add(layers.LeakyReLU(alpha=0.01))\n",
        "    model.add(layers.Conv2D(512, kernel_size=4, strides=2, padding=\"same\"))\n",
        "    model.add(layers.LeakyReLU(alpha=0.01))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
        "    return model\n",
        "\n",
        "def build_generator(latent_dim):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Dense(8 * 8 * 512, input_dim=latent_dim))\n",
        "    model.add(layers.Reshape((8, 8, 512)))\n",
        "    model.add(layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU(alpha=0.01))\n",
        "    model.add(layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU(alpha=0.01))\n",
        "    model.add(layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding=\"same\"))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU(alpha=0.01))\n",
        "    model.add(layers.Conv2DTranspose(32, kernel_size=4, strides=2, padding=\"same\"))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU(alpha=0.01))\n",
        "    model.add(layers.Conv2DTranspose(8, kernel_size=4, strides=2, padding=\"same\"))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU(alpha=0.01))\n",
        "    model.add(layers.Conv2DTranspose(3, kernel_size=4, strides=2, padding=\"same\", activation=\"sigmoid\"))\n",
        "    return model\n",
        "\n",
        "discriminator = build_discriminator()\n",
        "generator = build_generator(5000)\n",
        "\n",
        "generator_optimizer = keras.optimizers.Adam(learning_rate=0.00005, beta_1=0.5)\n",
        "discriminator_optimizer = keras.optimizers.Adam(learning_rate=0.00005, beta_1=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVOyHOEKDAQX"
      },
      "source": [
        "# Define loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "G9DEnkKLDB8A"
      },
      "outputs": [],
      "source": [
        "cross_entropy = keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjO4HZo7DC8a"
      },
      "source": [
        "# Define a function to show and save images generated during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ixAte7waDD0b"
      },
      "outputs": [],
      "source": [
        "def show_and_save_images(model, epoch, test_input):\n",
        "    predictions = model(test_input, training=False)\n",
        "\n",
        "    fig = plt.figure(figsize=(4, 4))\n",
        "    for i in range(predictions.shape[0]):\n",
        "        plt.subplot(4, 4, i + 1)\n",
        "        plt.imshow(predictions[i, :, :, :])\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.savefig(f\"training_log_img/epoch_{epoch + 1}.png\")\n",
        "    plt.show()\n",
        "    plt.clf()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qphk-f4gDFOk"
      },
      "source": [
        "# Training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "3zQObhjFDIMY"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(images):\n",
        "\n",
        "    noise = tf.random.normal([batch_size, 5000])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_images = generator(noise, training=True)\n",
        "\n",
        "        real_output = discriminator(images, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8XGNhqBDLSa"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPjan-3rDMuL",
        "outputId": "21068eda-b050-40c0-ab40-4ff3141f2fbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(X_train.shape[0]).batch(batch_size)\n",
        "\n",
        "epochs = 2000\n",
        "for epoch in range(epochs):\n",
        "    for image_batch in train_dataset:\n",
        "        train_step(image_batch)\n",
        "\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        show_and_save_images(generator, epoch, tf.random.normal([16, 5000]))\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}